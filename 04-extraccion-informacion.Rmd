---
title: "Métodos Computacionales para las Ciencias Sociales"
subtitle: "Extracción de información web"  
author: 
  - "Klaus Lehmann"
output:
  xaringan::moon_reader:
    css: xaringan-themer2.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.retina=3,
  out.width = "70%",
  cache = FALSE,
  echo = T,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
options(scipen = 999)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(tidyverse)
library(DT)
library(kableExtra)
library(plotly)
library(readr)
library(feather)
#style_duo_accent(
#  primary_color = "#1381B0",
#  secondary_color = "#FF961C",
#  inverse_header_color = "#FFFFFF"
#)
xaringanExtra::use_panelset()

```

## Contenidos de la clase

Uso de APIs para las ciencias sociales

Caso específico de Spotify

---

## Produciendo datos

.pull-left[

<img src="imagenes/ferrari.jpg" width="500" />
]

--

.pull-right[

<img src="imagenes/tractor.jpg" width="600" />
]

---

## Formatos y tecnologías de la web

.pull-left[

<img src="imagenes/ejemplo_json.png" width="450" />

<img src="imagenes/ejemplo_xml.png" width="250" />

]

.pull-right[

<img src="imagenes/ejemplo_html.png" width="200" />

<img src="imagenes/ejemplo_css.jpg" width="300" />

]

---

class: inverse center middle

# APIs


---

## ¿Qué es un API?

*Application programming interface* (interfaz de programación de aplicaciones)

--

Pieza de código que comunica 2 aplicaciones o sistemas

--

.center[
<img src="imagenes/puente.jpeg" width="300" />
]

Una API nos permite acceder a datos, sin enterarnos de la implementación que hay detrás

- ¿SQL?
- ¿Grafos?
- ¿mongodb?

*No nos importa*

---

## Características de una API

Recibe una petición (*request*) y devuelve una respuesta estructurada

La API manda al servidor la solicitud

Generalmente, la respuesta es un archivo json (ahondaremos en esto después)

--

Métodos más comunes:

- GET: obtener recurso
- POST: crear recurso o enviar datos
- PUT: actualizar recurso
- PATCH: actualizar recurso
- DELETE: eliminar recurso

--

Nosotros solo utilizaremos GET y POST

---

## Ejemplo de un request



.panelset[
.panel[.panel-name[curl]

Hacer *requests* correctamente implica conocer algunos protocolos

Estamos pidiendo todos los discos de Bruno Mars desde Spotify, usando curl desde la terminal


```{r, eval=F}
curl --request GET \
--url https://api.spotify.com/v1/artists/0du5cEVh5yTK9QJze8zA0C/albums \
--header 'Authorization: Bearer BQDV6qbwbL...AYc3Wja1fhyXVfbDZhnwGwXiSM4l1FZw7UoN12YtmSf0omuuBbH_rttJ7uzhFMuY'
```

]

.panel[.panel-name[httr]

En R podemos usar un paquete llamado httr, pero sigue siendo complejo 


```{r, eval=F}
by_httr <- GET(url = "https://api.spotify.com/v1/artists/0du5cEVh5yTK9QJze8zA0C/albums", 
             add_headers(Authorization="Bearer BQDV6qbwbLcun5HK6asPqYRWzvA....7UoN12YtmSf0omuuBbH_rttJ7uzhFMuY")
)
```


]


.panel[.panel-name[spotifyr]

Afortunadamente, hay personas que han construído paquetes 😁

```{r, eval=FALSE}
bruno_mars <- get_artist_albums(id = "0du5cEVh5yTK9QJze8zA0C")
```

Nos centraremos en esta estrategia

En la vida real tendrán que aprender APIs desde 0

]

]


---

## Algunas APIs para las ciencias sociales

En este [bookdown](https://bookdown.org/paul/apis_for_social_scientists) se describen varias APIs y aplicaciones para las ciencias sociales

Algunos ejemplos:

- youtube
- google news
- twitter (X)
- spotify
- google places

--

Revisaremos en detalle spotify

En la unidad de datos geoespaciales revisaremos *google places*


---

class: inverse center middle

# spotify

---

## Primeros pasos

[Pasos](https://developer.spotify.com/documentation/web-api) a seguir

1. Crear una cuenta en spotify (gratis o pagada)
2. Entrar a [spotify for developers](https://developer.spotify.com) 
3. Ir a la sección *dashboard* y crear una aplicación 
4. Dentro de la aplicación, ir a *settings* y buscar las credenciales (client id y client secret)

--

### Ahora podemos empezar a descargar datos

---

## Primeros pasos

```{r, eval=FALSE}
install.packages("spotifyr")
```

--

Con nuestras credenciales debemos activar un *token* que dura **una hora**

```{r crear token}
library(spotifyr)
#Crear token a partir de las credenciales
access_token <- get_spotify_access_token(client_id = Sys.getenv("SPOTIFY_CLIENT_ID"), 
                                         client_secret = Sys.getenv("SPOTIFY_CLIENT_SECRET"))
```

--

**Comentario al margen**

- Nunca deben escribir explícitamente sus credenciales en el código
- Una alternativa es usar variables de ambiente

---
## Identificadores spotify

Las APIs suelen funcionar con un sistema de identificadores:

- álbum
- artista
- mercado
- episodios

Podemos ver el id de un artista mediante la aplicación de escritorio

.center[
<img src="imagenes/spotify_logo.png" width="300" />
]

---

## Discos de Bruno Mars

```{r}
bruno_albums <-  get_artist_albums("0du5cEVh5yTK9QJze8zA0C", limit = 50)
```

```{r, echo=FALSE}
bruno_albums %>% 
  select(album_type, id, name, total_tracks) %>% 
  datatable(options = list(pageLength = 5, dom = "p") )
```


---

## Exploremos Unorthodox Jukebox

```{r}
jukebox <-  get_album_tracks("58ufpQsJ1DS5kq4hhzQDiI") # id del álbum
```

```{r, echo=FALSE}
jukebox %>% 
  select(duration_ms, id, name ) %>% 
  datatable(options = list(pageLength = 5, dom = "p") )
```

---

## Exploremos treasure

La API nos devuelve características del *track*

```{r}
treasure <- get_track_audio_features("55h7vJchibLdUkxdlX3fK7")
```

```{r, echo=FALSE}
treasure %>% 
  select(-c("id", "uri", "analysis_url", "time_signature", "track_href", "type") ) %>% 
  datatable(options = list(pageLength = 5, dom = "p") )

```

---

## Discografía completa

Todos los *tracks* de *Foo Fighters*, *Shakira* y *Slayer*

```{r shakira-foo-slayer}
foo_fighters <-  get_artist_audio_features("7jy3rLJdDQY21OgRLCZ9sD", include_groups = "album")
shakira <-  get_artist_audio_features("0EmeFodog0BfCgMzAIvKQp", include_groups = "album")
slayer <-  get_artist_audio_features("1IQ2e1buppatiN1bxUVkrk", include_groups = "album")
full <- bind_rows(foo_fighters, shakira, slayer)
```

```{r plot slayer, echo=FALSE, fig.height=5}

plot <- full %>% 
  group_by(artist_name) %>% 
  summarise_at(.vars = c("energy", "valence", "danceability"), .funs = mean ) %>% 
  pivot_longer(cols = c("energy", "valence", "danceability"), names_to = "indicator", values_to = "value") %>% 
  ggplot(aes(x = artist_name, y = value, fill = indicator )) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_bw() +
  theme(axis.title = element_blank())

ggplotly(plot)
```


---

## Explorando géneros

La función `get_genre_artists` permite hacer búsquedas con ciertos criterios

- género
- mercado

--

Es muy importante el sistema de **paginación**

--

En general, las APIs tienen mecanismos para no solicitar toda la información en una sola llamada

- limit: número de respuestas que queremos (máximo = 50)
- offset: índice desde el cual queremos partir (máximo = 1.000)


```{r}
bandas_chilenas1 <-  get_genre_artists(genre = "rock", market = "CL", limit = 5, offset = 0) # 0 al 4  
bandas_chilenas2 <-  get_genre_artists(genre = "rock", market = "CL", limit = 5, offset = 5) # 5 al 9  
```

--

Cuando sobrepasamos el 1.000, obtenemos un error

```{r, error=TRUE}
final <-  get_genre_artists(genre = "rock", market = "CL", limit = 2, offset = 999) # 999 + 2 = 1001  
```


---

## Discos de rock en Chile

Vamos a descargar información sobre canciones de rock dentro del mercado chileno

--

Debemos seguir varios pasos:

1. Buscar los identificadores de las bandas
2. Buscar los identificadores de los álbumes
3. Buscar los identificadores de las canciones
4. Descargar las características de las canciones

--

El paso 1 lo harán ustedes

--

Los pases 2, 3 y 4 los haremos en conjunto


---
## Paso 1: discos de rock en Chile

Utilizando el sistema de paginación, descarga toda las bandas de rock que participan en el mercado chileno

Deberías usar un *loop* para iterar sobre una secuencia de *offsets*

Almacenar la información en una lista puede ser buena idea

*Pista:* Podría servirles esta secuencia

```{r}
secuencia <-  seq(0, 950, 50)
```


```{r, eval=FALSE, echo=FALSE}
# Bonus track: podemos resolverlo con map de manera muy compacta
bandas_rock_df <- map(secuencia, ~get_genre_artists(genre = "rock", market = "CL", limit = 50, offset = .x)) %>% 
  bind_rows()
```


---

## Paso 2: Extraer identificadores

**Muestra de 150 bandas**

```{r, eval=FALSE}
set.seed(123)
library(feather)
bandas_sample <-  bandas_rock_df %>% 
  sample_n(150) %>% 
  select(id, name, popularity)
write_feather(bandas_sample, "data/muestra_bandas.feather")

```

Ahora buscamos la discografía para todas estas bandas

--


```{r, eval=FALSE}
datos <- list()
i <- 1 # iterador
# Para cada banda
for (id in bandas_sample$id) {
  # Descargar discografía completa 
  datos[[i]] <-  get_artist_audio_features(id, include_groups = "album") #<<
  i <- i + 1
}
# Combinar todo en una tabla
final <- datos %>% 
  bind_rows()
write_csv(final, "data/bandas_mercado_chileno_sample.csv")
```


**Este código podría funcionar, pero hay 2 consideraciones**
- función get_artist_albums tiene por defecto 20 álbumes
- Pueden pasar cosas inesperadas
- Cuota máxima de la API


---

##  Restricciones de la API

.center[
<img src="imagenes/rates.png" width="700" />
]

La función `get_artist_audio_features`, internamente, hace muchos *requests*

```{r, eval=FALSE}
# para cada artista
artista
  # Busca sus álbumes
  album
    # Busca tracks dentro de cada álbum
    tracks
      # Busca características para cada canción
      features
```

--

### Si pedimos todas las bandas, es muy probable que sobrepasemos la cuota :(

---

## Código "real"


```{r, eval=FALSE}
datos <- list()
i <- 1 # iterador
# Para cada banda
for (id in bandas_sample$id) {
  # Atrapar errores
  out <- tryCatch(
    {
      get_artist_audio_features(id, include_groups = "album") # solo primeros 20 álbumes
    },
    error=function(cond) {
      message(paste("No albums found with artist_id", id))
      return(NA)
    }
  )
  datos[[i]] <-  out
  i <- i + 1
}

```

Debemos dormir estratégicamente

```{r, eval=FALSE}
# 5 segundos cada 50 requests
if (n_requests %% 50 == 0) {
    print("durmiendo")
    Sys.sleep(5)
}    
      

```

---

## Analicemos los datos

.panelset[
.panel[.panel-name[álbumes]

```{r, fig.height=5, fig.align='center'}
bandas_sample <- read_feather("data/muestra_bandas.feather")
discografia <- read_csv("data/bandas_mercado_chileno_sample.csv")
print(nrow(discografia))

n_discos <- discografia %>% 
  filter(album_release_year > 0 & album_release_year < 2023) %>% 
  group_by(album_release_year, album_id) %>% 
  slice(1) %>%
  group_by(album_release_year) %>% 
  summarise(n = n())

n_discos %>% 
  ggplot(aes(x = album_release_year, n, group = 1)) +
  geom_line() +
  theme_bw()

```


]



.panel[.panel-name[características]

```{r, fig.height=5, fig.align='center'}
discografia_long <-  discografia %>% 
  filter(album_release_year > 0 ) %>% 
  group_by(album_release_year) %>%
  summarise_at(.vars = c("danceability", "energy", "valence", "tempo", "tempo", "duration_ms"), .funs =  mean) %>% 
  pivot_longer(cols = -album_release_year, names_to = "var", values_to = "value")

discografia_long %>% 
  filter(var %in% c("danceability", "energy", "valence") ) %>% 
  filter(album_release_year < 2023) %>% 
  ggplot(aes(x = album_release_year, y = value, group = var, color = var)) +
  geom_line() +
  theme_bw()

```


]


.panel[.panel-name[tempo]

```{r, fig.height=5, fig.align='center'}
discografia_long %>% 
  filter(album_release_year < 2023) %>% 
  filter(var == "tempo") %>% 
  ggplot(aes(x = album_release_year, y = value, group = 1)) +
  geom_line() +
  theme_bw()
```


]


.panel[.panel-name[duración]

```{r, fig.height=5, fig.align='center'}
discografia_long %>% 
  filter(album_release_year < 2023) %>% 
  filter(var == "duration_ms") %>% 
  ggplot(aes(x = album_release_year, y = value, group = 1)) +
  geom_line() +
  theme_bw()

```

]
]


---

## Ejercicio: características, según popularidad

Queremos comparar las características de las bandas populares y poco populares

- danceability
- energy
- valence
- tempo
- duration_ms

1. Convertir la variable popularidad en tramos, mediante quintiles (`mutate` + `ntile`)
2. Calcular la media de las características para cada banda (`group_by` + `summarise`)
3. Unir la información de características con la popularidad de la banda (`left_join`)
4. Calcular la media a nivel en cada tramo de popularidad  (`group_by` + `summarise`)
5. Crear un gráfico de barras con los valores de las características, según tramo de popularidad (`ggplot2`)


El *dataframe* *bandas_sample* contiene la información de popularidad

El *dataframe* discografía contiene los datos de las bandas 


---

class: inverse center middle

# reddit

---

## API Reddit

.center[
<img src="imagenes/reddit-logo.png" width="400" />
]

--

[Aquí](https://www.reddit.com/dev/api/oauth) hay documentación de su API

--

La documentación no es buena 😥

---

## Praw

Usaremos **PRAW** (Python Reddit API Wrapper) de python

--

Es mucho más fácil que usar directamente la API 😀

[Aquí](https://praw.readthedocs.io/en/stable/index.html#) está la documentación


---

## Primeros pasos

Debemos tener una cuenta en reddit e inscribir una aplicación

[Aquí](https://rymur.github.io/setup) están las instrucciones para crear una app

--

.center[
<img src="imagenes/app-creation.png" width="600" />
]



---

## Autenticación con PRAW 1


```{python, message = F, warning = FALSE}

import praw 
import pandas as pd # para manipulación de dataframes
from dotenv import load_dotenv # cargar variables de ambiente
import os # cargar variables de ambiente

load_dotenv()

client_id = os.getenv('CLIENT_ID')
client_secret = os.getenv('CLIENT_SECRET')
username = os.getenv('USERNAME')
password = os.getenv('PASSWORD')
user_agent = os.getenv('USER_AGENT')
auth_url = "https://www.reddit.com/api/v1/access_token"

```

---

## Autenticación con PRAW 2

La función Reddit del praw hace la autenticación

```{python}
reddit = praw.Reddit(
    client_id=client_id,
    client_secret=client_secret,
    user_agent=user_agent
)
```

---
## Descargando datos 1

Entramos al subreddit *bikepacking* y extraemos las dos últimas publicaciones

```{python}
for submission in reddit.subreddit("bikepacking").new(limit = 2):
    print(submission.title)
```

---

## Descargando datos 2

Contamos con varios métodos

- new
- hot
- top
- random
- controversial
- rising

```{python}
for submission in reddit.subreddit("Rlanguage").new(limit = 2):
    print(submission.fullname, submission.title)
```



---

## Quiero extraer más publicaciones

Usamos el subreddit *Rlanguage* y no ponemos límite

```{python, eval = FALSE}
posts = []
for submission in reddit.subreddit("Rlanguage").top(limit = None):
    data =  [submission.id, submission.title, submission.selftext, submission.score ]
    posts.append(data)

df_r = pd.DataFrame(posts, columns=['id', 'title', 'selfttext', "score"])
```

```{python, echo = FALSE}
df_r = pd.read_parquet("data/r_submissions.parquet")

```


---

## Límite de la API

La API no nos deja descargar más de 1.000 comentarios

```{python}
df_r.shape
```


---

## Podemos ir a buscar más atrás

Fijamos varias ventanas de tiempo

```{python, eval = FALSE}

time_filters = ['day', 'week', 'month', 'year', 'all']
posts = []
subreddit = reddit.subreddit('Rlanguage')

# Para cada ventana de tiempo
for time_filter in time_filters:
    # Extraemos los 1.000 primeros mensajes     
    submissions = subreddit.top(time_filter=time_filter, limit=1000)
    for submission in submissions:
        data =  [submission.fullname, submission.title, submission.selftext, submission.score ]
        posts.append(data)

df_r = pd.DataFrame(posts, columns=['id', 'title', 'selfttext', "score"])

```


```{python, echo = FALSE, eval = FALSE}
df_r.to_parquet("data/r_submission_big.parquet")
```

```{python, echo = FALSE}
df_r = pd.read_parquet("data/r_submission_big.parquet")

```

---

## Resultado

Obtenemos un poco más, pero... 

```{python}
df_r.shape
```

--

... al mismo tiempo tenemos duplicados

```{python}

import numpy as np
len(np.unique(df_r.id))

```

---

## Probemos con comentarios de python

```{python}
time_filters = ['day', 'week', 'month', 'year', 'all']
posts = []
subreddit = reddit.subreddit('Python')

# Para cada ventana de tiempo
for time_filter in time_filters:
    # Extraemos los 1.000 primeros mensajes     
    submissions = subreddit.top(time_filter=time_filter, limit=1000)
    for submission in submissions:
        data =  [submission.fullname, submission.title, submission.selftext, submission.score ]
        posts.append(data)

df_python = pd.DataFrame(posts, columns=['id', 'title', 'selfttext', "score"])
```

```{python, echo = FALSE, eval = FALSE}
df_python.to_parquet("data/python_submission_big.parquet")
```


```{python, echo = FALSE}
df_python = pd.read_parquet("data/python_submission_big.parquet")

```


```{python}
print(df_python.shape)
print(len(np.unique(df_python.id)))

```

---
## Comparemos


```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(arrow)
library(quanteda)
library(quanteda.textstats)
library(dplyr)

python_comments <- read_parquet("data/python_submission_big.parquet")
r_comments <- read_parquet("data/r_submission_big.parquet")

dataset <- python_comments %>% 
  mutate(language = "python") %>% 
  bind_rows(r_comments %>% 
              mutate(language = "r")
              )

corpus_python <- dataset %>% 
  select(text = title,language ) %>% 
  quanteda::corpus()

dfm_python <- tokens(corpus_python, remove_punct = TRUE) %>% 
  tokens_remove(stopwords()) %>% 
  dfm()


textstat_frequency(dfm_python, n = 5, groups = language) %>% 
  select(feature, frequency,group) %>% 
  head(10) %>% 
  DT::datatable()



```

---

## Idea

Podemos planificar descargas periódicas

Ejemplo: Descargar una vez a la semana durante 2 meses

Así, podemos descargar una gran cantidad de información

---

## En resumen

*Spotify* y *Reddit* son casos particulares de uso de APIs

Cada API tiene sus particularidades

La idea de esta clase ha sido aprender la mecánica general de las APIs

--

**En la vida real, muchas veces no contamos con paquetes**

**Y tenemos que relacionarnos directamente con los *endpoints* **

---
class: center, middle

## Métodos Computacionales para las Ciencias Sociales

### Hasta la próxima clase


